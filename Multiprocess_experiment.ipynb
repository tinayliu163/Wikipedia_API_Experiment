{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/64702600\n",
    "\n",
    "https://www.cnblogs.com/654321cc/p/7683238.html\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/46798399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right way to read data not to cause memory to overflow\n",
    "\n",
    "#read the data in line by line so that only a single line is stored in the RAM at any given time\n",
    "import os\n",
    "root = '/srv/spark/shared-data-export/liuy/datasets/video_level_classification/Video_game_extraction/Access_wikidata'\n",
    "\n",
    "with open(os.path.join(root, 'wikiListOfArticles_nongame.txt')) as f:\n",
    "    for line in f:\n",
    "        process(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Process类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***函数 multiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)***\n",
    "\n",
    "group为预留参数。\n",
    "\n",
    "target为可调用对象（函数对象），为子进程对应的活动；相当于multiprocessing.Process子类化中重写的run()方法。\n",
    "\n",
    "name为线程的名称，默认（None）为\"Process-N\"。\n",
    "\n",
    "args、kwargs为进程活动（target）的非关键字参数、关键字参数。\n",
    "\n",
    "deamon为bool值，表示是否为守护进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试Python多进程\n",
      "测试Python多进程\n",
      "测试Python多进程\n",
      "测试Python多进程\n",
      "测试Python多进程\n",
      "结束测试\n"
     ]
    }
   ],
   "source": [
    "# main part\n",
    "\n",
    "\"\"\"\n",
    "def f(a, b = value):\n",
    "    pass\n",
    "\n",
    "p = multiprocessing.Process(target = f, args = (a,), kwargs = {b : value}) \n",
    "p.start()\n",
    "p.join()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from multiprocessing import  Process\n",
    "\n",
    "def fun1(name):\n",
    "    print('测试%s多进程' %name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_list = []\n",
    "    for i in range(5):  #开启5个子进程执行fun1函数\n",
    "        p = Process(target=fun1,args=('Python',)) #实例化进程对象 #创建子进程\n",
    "        p.start() #启动进程活动, p为进程实例\n",
    "        process_list.append(p)\n",
    "\n",
    "    for i in process_list:\n",
    "        p.join() #使主调进程（包含XXX.join()语句的进程）阻塞，直至被调用进程p运行结束或超时（如指定timeout)\n",
    "\n",
    "    print('结束测试')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589 from foo\n",
      "1592 from bar\n",
      "1482 over\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "import os\n",
    "def foo():\n",
    "    print('%s from foo'%os.getpid())\n",
    "def bar():\n",
    "    print('%s from bar' % os.getpid())\n",
    "if __name__ == '__main__':\n",
    "    p1=Process(target=foo)\n",
    "    p2=Process(target=bar)\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    print('%s over'%os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pool类：\n",
    "\n",
    "***如果要启动大量的子进程，可以用进程池的方式批量创建子进程：Pool类***\n",
    "\n",
    "\n",
    "将输入映射到不同的CPU, 并收集所有CPU的输出。等待所有任务完成后，然后返回输出。\n",
    "\n",
    "输出为：以列表或者数组的形式返回输出\n",
    "\n",
    "执行中的进程存储在存储器中，而其他非执行进程则存储在存储器之外。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***multiprocessing.Pool(processes=None, initializer=None, initargs=(), maxtasksperchild=None)***\n",
    "\n",
    "processes ：使用的工作进程的数量，如果processes是None那么使用 os.cpu_count()返回的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run task 3 (1598)...\n",
      "Run task 1 (1596)...\n",
      "Run task 4 (1599)...\n",
      "Run task 2 (1597)...\n",
      "Run task 5 (1600)...\n",
      "Run task 6 (1601)...\n",
      "Run task 7 (1602)...\n",
      "Task 4 runs 0.01 seconds.\n",
      "Run task 8 (1599)...\n",
      "Run task 0 (1595)...\n",
      "Task 0 runs 0.10 seconds.\n",
      "Run task 9 (1595)...\n",
      "Task 6 runs 0.23 seconds.\n",
      "Task 2 runs 0.83 seconds.\n",
      "Task 1 runs 1.02 seconds.\n",
      "Task 5 runs 1.12 seconds.\n",
      "Task 3 runs 1.56 seconds.\n",
      "Task 8 runs 1.65 seconds.\n",
      "Task 9 runs 1.88 seconds.\n",
      "Task 7 runs 2.31 seconds.\n",
      "结束测试\n"
     ]
    }
   ],
   "source": [
    "#main part\n",
    "\n",
    "\"\"\"\n",
    "def f(a, b = value):\n",
    "    pass\n",
    "\n",
    "pool = multiprocessing.Pool() \n",
    "pool.apply_async(f, args = (a,), kwds = {b : value})\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from  multiprocessing import Process,Pool\n",
    "import os, time, random\n",
    "\n",
    "def fun1(name):\n",
    "    print('Run task %s (%s)...' % (name, os.getpid()))\n",
    "    start = time.time()\n",
    "    time.sleep(random.random() * 3)\n",
    "    end = time.time()\n",
    "    print('Task %s runs %0.2f seconds.' % (name, (end - start)))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pool = Pool(8) #创建一个6个进程的进程池, 即6个cores\n",
    "\n",
    "    for i in range(10): ##开启10个子进程执行fun1函数\n",
    "        pool.apply_async(func=fun1, args=(i,)) \n",
    "        #apply_async对应的每个子进程是异步执行的（即并行）。异步执行指的是一批子进程并行执行，且子进程完成一个，就新开始一个，而不必等待同一批其他进程完成\n",
    "\n",
    "    pool.close() #关闭进程池，关闭后不能往pool中增加新的子进程，然后可以调用join()函数等待已有子进程执行完毕。\n",
    "    pool.join() ##等待进程池中的子进程执行完毕。需在close()函数后调用。\n",
    "    print('结束测试')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646 from foo\n",
      "1647 from bar\n",
      "<multiprocessing.pool.ApplyResult object at 0x7fa0b4d2d9e8>\n",
      "2.0118601322174072\n",
      "foo\n",
      "bar\n",
      "0.00010599999999993948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/__main__.py:20: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel/__main__.py:23: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import os,time\n",
    "def foo():\n",
    "    time.sleep(1)\n",
    "    print('%s from foo'%os.getpid())\n",
    "    return 'foo'\n",
    "def bar():\n",
    "    time.sleep(2)\n",
    "    print('%s from bar' % os.getpid())\n",
    "    return 'bar'\n",
    "if __name__ == '__main__':\n",
    "    p=Pool(8)\n",
    "    t1=time.time()\n",
    "    res1=p.apply_async(foo) #给出foo()函数结果\n",
    "    res2=p.apply_async(bar) #给出bar()函数结果\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print(res1)\n",
    "    print(time.time()-t1)          ##多出来的0.15秒是开启进程所花费的时间\n",
    "    t2=time.clock()\n",
    "    print(res1.get())\n",
    "    print(res2.get())\n",
    "    print(time.clock()-t2)\n",
    "\n",
    "#先运行子进程res1 and res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "def test(p):\n",
    "       print(p)\n",
    "       time.sleep(3)\n",
    "if __name__==\"__main__\":\n",
    "    pool = Pool(processes=10)\n",
    "    for i  in range(500):\n",
    "        '''\n",
    "        ('\\n'\n",
    "         '\t（1）遍历500个可迭代对象，往进程池放一个子进程\\n'\n",
    "         '\t（2）执行这个子进程，等子进程执行完毕，再往进程池放一个子进程，再执行。（同时只执行一个子进程）\\n'\n",
    "         '\t for循环执行完毕，再执行print函数。\\n'\n",
    "         '\t')\n",
    "        '''\n",
    "        pool.apply(test, args=(i,))   #维持执行的进程总数为10，当一个进程执行完后启动一个新进程.\n",
    "    print('test')\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool的 map 方法\n",
    "\n",
    "如果子进程有返回值，且返回值需要集中处理，则建议采用map方式（子进程活动只允许1个参数）\n",
    "\n",
    "***map在爬虫的领域里也可以使用，比如多个URL的内容爬取，可以把URL放入元祖里，然后传给执行函数。***\n",
    "\n",
    "例如 https://medium.com/@Sean_Hsu/concurrency-parallelism-in-python-ebdc040e0881\n",
    "\n",
    "#### 如何使用Pool Map：\n",
    "\n",
    "如果要执行一百万个任务，则可以创建一个池，其中包含与CPU内核一样多的进程数，然后将百万个任务列表传递给pool.map，池将这些任务分发给工作进程 (通常与可用内核的数量相同），并以列表的形式收集返回值并将其传递给父进程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***对map的理解：***\n",
    "\n",
    "pool.map() will loop over the file and deliver lines to the worker function. \n",
    "\n",
    "Map blocks and returns the entire result when its done.\n",
    "\n",
    "the pool.map() is going to read your entire file into memory all at once before dishing out work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07900619506835938\n",
      "2.0829520225524902\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] <class 'list'>\n",
      "2.1894869804382324\n"
     ]
    }
   ],
   "source": [
    "#map()\n",
    "\n",
    "\"\"\"\n",
    "XXX.map(func, iterable, chunksize=None) \n",
    "#将iterable的每个元素作为参数，应用func函数，返回函数结果组成的list，阻塞版本。func(iterable[i])为子进程对应的活动。XXX为进程池实例。\n",
    "#chunksize\n",
    "\n",
    "map(func, iterable[, chunksize])\n",
    "\n",
    "This method chops the iterable into a number of chunks which it submits to the process pool as separate tasks. \n",
    "The (approximate) size of these chunks can be specified by setting chunksize to a positive integer.\n",
    "\n",
    "I presume a process picks up the next chunk from a queue when done with previous chunk.\n",
    "\n",
    "The default chunksize depends on the length of iterable and is chosen \n",
    "so that the number of chunks is approximately four times the number of processes.\n",
    "\n",
    "def f(a): #map方法只允许1个参数\n",
    "    pass\n",
    "\n",
    "pool = multiprocessing.Pool() \n",
    "result = pool.map_async(f, (a0, a1, ...)).get() #XXX.map()的异步（并行）版本，返回MapResult实例（其具有get()方法，获取结果组成的list）\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def foo(x):\n",
    "    time.sleep(1)\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    l = [1,2,3,4,5,6,7,8,9,10]\n",
    "    t1 = time.time()\n",
    "    p = Pool(6)\n",
    "    print(time.time()-t1)\n",
    "    res = p.map(foo,l) # 这行代码表示所有的进行都已经执行完了，并且每个进程的结果都拿到，放在了res中 #l是foo()的参数\n",
    "    print(time.time()-t1)   \n",
    "    print(res,type(res)) #res 存储结果\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用concurrent.futures 库内的ProcessPoolExecutor 来创建多进程\n",
    "\n",
    "concurrent.futures為python集合thread和process的高階API，能夠讓使用者更容易的使用這進（線）程，而且能夠方便的得到函式的回傳值，先前的thread及multiprocessing要取得return值稍稍麻煩了一點。但使用concurrent的executor提供的map及submit方法能夠快速地取得個函式的回傳值。以下會提供map及submit的使用方式，兩者在thread及process的情況下階相同。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "executor=ProcessPoolExecutor()：生成一个ProcessPoolExecutor对象；\n",
    "\n",
    "future=executor.submit():提交任务，返回一个Future对象。\n",
    "\n",
    "executor.shutdown()。相当于Pool类中的close()和join()\n",
    "\n",
    "future.result()：从Future对象中获取其返回值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executor <concurrent.futures.process.ProcessPoolExecutor object at 0x7fa0d497d358>\n",
      "<Future at 0x7fa0d4987ef0 state=running> <Future at 0x7fa0d49062e8 state=running>\n",
      "1670  MainThread from foo\n",
      "1671  MainThread from bar\n",
      "foo\n",
      "bar\n",
      "2.0878539085388184\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor\n",
    "import os,time\n",
    "from threading import current_thread\n",
    "def foo():\n",
    "    time.sleep(1)\n",
    "    print('%s  %s from foo'%(os.getpid(),current_thread().getName()))\n",
    "    return 'foo'\n",
    "def bar():\n",
    "    time.sleep(2)\n",
    "    print('%s  %s from bar' % (os.getpid(),current_thread().getName()))\n",
    "    return 'bar'\n",
    "if __name__ == '__main__':\n",
    "    t1=time.time()\n",
    "    executor=ProcessPoolExecutor()\n",
    "    print('executor',executor)\n",
    "    future_to_url = {}\n",
    "    future1=executor.submit(foo) #foo函数不用传入参数\n",
    "    future2=executor.submit(bar)\n",
    "    print(future1,future2)\n",
    "    executor.shutdown()\n",
    "    print(future1.result())\n",
    "    print(future2.result())\n",
    "    print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<multiprocessing.pool.Pool at 0x7fa0d496c240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "p = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "# result = p.map(eval_formula, expression_list)\n",
    "# p.close()\n",
    "# p.join()\n",
    "\n",
    "# def eval_formula(formula):\n",
    "#     #evaluates the expression\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用python的多进程详细步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import trange                  # 显示进度条\n",
    "from multiprocessing import cpu_count    # 查看cpu核心数\n",
    "from multiprocessing import Pool         # 并行处理必备，进程池 \n",
    " \n",
    "# import cv2                                \n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np                        \n",
    "# from contrast import ImageContraster     # 自定义图像处理库，忽略\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2、定义单线程的工作流函数，换句话说就是单线程需要完成的任务***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单线程所进行的自定义任务，每个任务可能不同，所以以下代码没有太大的参考价值\n",
    "def single_worker(List_imgs, src_path, dest_path):\n",
    "    icter = ImageContraster()\n",
    "        \n",
    "    # 注意：\n",
    "    # 这里使用的是 trange 而不是 range 原因是为了美观\n",
    "    # trange 会输出一个进度条\n",
    "    # 当然，你也可以换成 range 函数\n",
    "    for i in trange(len(List_imgs)):\n",
    "        if not List_imgs[i].endswith('.jpg'):\n",
    "            continue\n",
    "        file = List_imgs[i]\n",
    "        filepath = os.path.join(src_path, file)\n",
    "        img = cv2.imread(filepath)\n",
    "        he_eq_img = icter.enhance_contrast(img, method=\"HE\")\n",
    "        he_eq_img = np.array(he_eq_img)\n",
    "        save_he = os.path.join(dest_path, file)\n",
    "        cv2.imwrite(save_he, he_eq_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3、将总数据集进行分割，分割成子数据集 (分割成chunks）***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据CPU核数来划分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略\n",
    "SourceImgs = './data/source/'\n",
    "HE_path = \"./data/he_trainval_jpg/\"\n",
    "\n",
    "# if not os.path.exists(HE_path):\n",
    "#     os.mkdir(HE_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cores = cpu_count()\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下重点\n",
    "#将数据集下的图片名加载到List_imgs中\n",
    "List_imgs = os.listdir(SourceImgs)\n",
    "Len_imgs = len(List_imgs) # 数据集长度\n",
    "num_cores = cpu_count()   # cpu核心数\n",
    "\n",
    " # 双核，将所有数据集分成两个子数据集\n",
    "if num_cores == 2:\n",
    "    subset1 = List_imgs[:Len_imgs // 2]\n",
    "    subset2 = List_imgs[Len_imgs // 2:]\n",
    " \n",
    "    List_subsets = [subset1, subset2]\n",
    "\n",
    "    # 四核，将所有数据集分成四个子数据集\n",
    "\n",
    "elif num_cores == 4:  \n",
    "    subset1 = List_imgs[:Len_imgs // 4]\n",
    "    subset2 = List_imgs[Len_imgs // 4: Len_imgs // 2]\n",
    "    subset3 = List_imgs[Len_imgs // 2: (Len_imgs * 3) // 4]\n",
    "    subset4 = List_imgs[(Len_imgs * 3) // 4:]\n",
    " \n",
    "    List_subsets = [subset1, subset2, subset3, subset4]\n",
    "      \n",
    "    # 八核以上，将所有数据集分成八个子数据集\n",
    "elif num_cores >= 8:     \n",
    "    num_cores = 8\n",
    "    subset1 = List_imgs[:Len_imgs // 8]\n",
    "    subset2 = List_imgs[Len_imgs // 8: Len_imgs // 4]\n",
    "    subset3 = List_imgs[Len_imgs // 4: (Len_imgs * 3) // 8]\n",
    "    subset4 = List_imgs[(Len_imgs * 3) // 8: Len_imgs // 2]\n",
    "    subset5 = List_imgs[Len_imgs // 2: (Len_imgs * 5) // 8]\n",
    "    subset6 = List_imgs[(Len_imgs * 5) // 8: (Len_imgs * 6) // 8]\n",
    "    subset7 = List_imgs[(Len_imgs * 6) // 8: (Len_imgs * 7) // 8]\n",
    "    subset8 = List_imgs[(Len_imgs * 7) // 8: ]\n",
    " \n",
    "    List_subsets = [subset1,subset2,subset3,subset4,\n",
    "                    subset5,subset6,subset7,subset8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4、开启多线程处理数据***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开辟进程池，不需要改动\n",
    "# num_cores为cpu核心数，也就是开启的进程数\n",
    "p = Pool(num_cores)   \n",
    " \n",
    "#\n",
    "# 对每个进程分配工作\n",
    "for i in range(num_cores):\n",
    "    #\n",
    "    # 格式：p.apply_async(task, args=(...))\n",
    "    # task：当前进程需要进行的任务/函数，只需要填写函数名\n",
    "    # args：task函数中所需要传入的参数\n",
    "    # 注意看 List_subsets[i] 就是传入不同的数据子集\n",
    "    p.apply_async(single_worker, args=(List_subsets[i], SourceImgs, HE_path))\n",
    "    res = p.map(single_worker, args=(List_subsets[i], SourceImgs, HE_path))\n",
    " # 当进程完成时，关闭进程池\n",
    "# 以下两行代码不需要改动\n",
    "p.close()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.blopig.com/blog/2016/08/processing-large-files-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wikiListOfArticles_nonredirects.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e3f1a773a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# use itertools to pull out chunks of lines as we need them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wikiListOfArticles_nonredirects.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wikiListOfArticles_nonredirects.txt'"
     ]
    }
   ],
   "source": [
    "#重新写分块的function\n",
    "\n",
    "#reading the data in line by line, so that only a single line is stored in the RAM at any given time.\n",
    "#main part\n",
    "#process： 是执行函数，需要自己写的\n",
    "#这里是process的line\n",
    "# generate a pool of workers, ideally one for each core, 即8个工人，one for processing each line\n",
    "# this still will run into memory problems\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "#init objects\n",
    "pool = mp.Pool(8) #generate \n",
    "jobs = []\n",
    "\n",
    "#create jobs\n",
    "with open(\"input.txt\") as f:\n",
    "    for line in f:\n",
    "        jobs.append( pool.apply_async(process,(line)) )\n",
    "\n",
    "#wait for all jobs to finish\n",
    "for job in jobs:\n",
    "    job.get()\n",
    "\n",
    "#clean up\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上面可以改进为\n",
    "#change the function to include opening the file, locating the specified line, reading it into memory, and then processing it\n",
    "#lineID is line number, thereby preventing the memory overflow\n",
    "#overhead 的问题：\n",
    "#the overhead involved in having to locate the line by reading iteratively through the file for each job is untenable,\n",
    "#getting progressively more time consuming as you get further into the file. \n",
    "def process_wrapper(lineID):\n",
    "    with open(\"input.txt\") as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i != lineID:\n",
    "                continue\n",
    "            else:\n",
    "                process(line)\n",
    "                break\n",
    "\n",
    "#init objects\n",
    "pool = mp.Pool(8)\n",
    "jobs = []\n",
    "\n",
    "#create jobs\n",
    "with open(\"input.txt\") as f:\n",
    "    for ID,line in enumerate(f):\n",
    "        jobs.append(pool.apply_async(process_wrapper,(ID)))\n",
    "\n",
    "#wait for all jobs to finish\n",
    "for job in jobs:\n",
    "    job.get()\n",
    "\n",
    "#clean up\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上面可以改进为：\n",
    "# 解决overhead的问题：\n",
    "#seek function of file: use the seek function of file objects which skips you to a particular location within a file. \n",
    "#tell function: Combining with the tell function, which returns the current location within a file\n",
    "#no need to iterate through the file line by line\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "def process_wrapper(lineByte):\n",
    "    with open(\"input.txt\") as f:\n",
    "        f.seek(lineByte)\n",
    "        line = f.readline()\n",
    "        process(line)\n",
    "\n",
    "#init objects\n",
    "pool = mp.Pool(cores)\n",
    "jobs = []\n",
    "\n",
    "#create jobs\n",
    "with open(\"input.txt\") as f:\n",
    "    nextLineByte = f.tell()\n",
    "    for line in f:\n",
    "        jobs.append(pool.apply_async(process_wrapper,(nextLineByte)) )\n",
    "        nextLineByte = f.tell() #returns the current location within a file\n",
    "\n",
    "#wait for all jobs to finish\n",
    "for job in jobs:\n",
    "    job.get()\n",
    "\n",
    "#clean up\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using seek we can move directly to the correct part of the file, whereupon we read a line into the memory and process it. We have to be careful to correctly handle the first and last lines, but otherwise this does exactly what we set out, namely using all the cores to process a given file while not overflowing the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***process multiple lines of the file at a time as a chunk***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wikiListOfArticles_nonredirects.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a311121ba432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#create jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunkStart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunkSize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunkify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wikiListOfArticles_nonredirects.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_wrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunkStart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunkSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a311121ba432>\u001b[0m in \u001b[0;36mchunkify\u001b[0;34m(fname, size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchunkify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfileEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mchunkEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wikiListOfArticles_nonredirects.txt'"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp,os\n",
    "\n",
    "def process_wrapper(chunkStart, chunkSize):\n",
    "    with open(\"wikiListOfArticles_nonredirects.txt\") as f:\n",
    "        f.seek(chunkStart)\n",
    "        lines = f.read(chunkSize).splitlines()\n",
    "        for line in lines:\n",
    "            process(line)\n",
    "\n",
    "def chunkify(fname,size=1024*1024):\n",
    "    fileEnd = os.path.getsize(fname)\n",
    "    with open(fname,'r') as f:\n",
    "        chunkEnd = f.tell()\n",
    "    while True:\n",
    "        chunkStart = chunkEnd\n",
    "        f.seek(size,1)\n",
    "        f.readline()\n",
    "        chunkEnd = f.tell()\n",
    "        yield chunkStart, chunkEnd - chunkStart\n",
    "        if chunkEnd > fileEnd:\n",
    "            break\n",
    "\n",
    "#init objects\n",
    "pool = mp.Pool(8)\n",
    "jobs = []\n",
    "\n",
    "#create jobs\n",
    "for chunkStart,chunkSize in chunkify(\"wikiListOfArticles_nonredirects.txt\"):\n",
    "    jobs.append(pool.apply_async(process_wrapper,(chunkStart,chunkSize)) )\n",
    "\n",
    "#wait for all jobs to finish\n",
    "for job in jobs:\n",
    "    job.get()\n",
    "\n",
    "#clean up\n",
    "pool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from lxml import html\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as sparkfn\n",
    "from pyspark.sql.types import *\n",
    "from  multiprocessing import Process,Pool, cpu_count\n",
    "import os, time, random\n",
    "import itertools\n",
    "from functools import reduce \n",
    "from pyspark.sql import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(title_list):\n",
    "\n",
    "    lst_categories = []\n",
    "    lst_page_id = []\n",
    "    lst_links = []\n",
    "    lst_externallinks = []\n",
    "    lst_sections = []\n",
    "    lst_redirects = []\n",
    "    lst_texts = []\n",
    "    lst_page_titles = []\n",
    "\n",
    "\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    for title in title_list:\n",
    "        PARAMS = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"redirects\": True\n",
    "    \n",
    "    }   \n",
    "        json_return = requests.get(url=URL, params=PARAMS)\n",
    "        json_data = json_return.json()\n",
    "\n",
    "        if 'parse' in json_data.keys():\n",
    "            #page_id\n",
    "            page_id = json_data['parse']['pageid']\n",
    "    \n",
    "            #categories\n",
    "            categories = []\n",
    "            for item in json_data['parse']['categories']:\n",
    "                if 'hidden' not in item.keys():\n",
    "                    label = item[\"*\"]\n",
    "                    categories.append(label)  \n",
    "    \n",
    "            #links\n",
    "            links = [item['*'] for item in json_data['parse']['links']]\n",
    "    \n",
    "            #external links\n",
    "            external_links = json_data['parse']['externallinks']\n",
    "    \n",
    "            #sections\n",
    "            sections = []\n",
    "    \n",
    "            for item in json_data['parse']['sections']:\n",
    "                section_name = item['line']\n",
    "                sections.append(section_name)\n",
    "        \n",
    "            lst_page_id.append(page_id)\n",
    "            lst_categories.append(categories)\n",
    "            lst_links.append(links)\n",
    "            lst_externallinks.append(external_links)\n",
    "            lst_sections.append(sections) \n",
    "            \n",
    "            #redirects\n",
    "            if len(json_data['parse']['redirects']) > 0:\n",
    "                lst_redirects.append(json_data['parse']['redirects'][0]['to'])\n",
    "            else:\n",
    "                lst_redirects.append(None)\n",
    "            \n",
    "            #texts\n",
    "            raw_html = json_data['parse']['text']['*']\n",
    "            document = html.document_fromstring(raw_html)\n",
    "            # redirect pages\n",
    "            #if len(para) > 0 and para[0].text_content().startswith(\"Redirect to\") is False:\n",
    "            text = \"\"    \n",
    "            for idx in range(len(document.xpath('//p'))):\n",
    "                text = text + \" \" + str(document.xpath('//p')[idx].text_content())\n",
    "                text = text.replace(\"\\n\", \".\")\n",
    "            \n",
    "            lst_texts.append(text)\n",
    "            lst_page_titles.append(title)\n",
    "\n",
    "    mySchema = StructType([StructField(\"page_id\", StringType(), True)\\\n",
    "                       ,StructField(\"page_title\", StringType(), True)\\\n",
    "                       ,StructField(\"page_text\", StringType(), True)\\\n",
    "                       ,StructField(\"category\", ArrayType(StringType()), True)\\\n",
    "                       ,StructField(\"links\", ArrayType(StringType()), True)\\\n",
    "                       ,StructField(\"external_links\", ArrayType(StringType()), True)\\\n",
    "                       ,StructField(\"sections\", ArrayType(StringType()), True)\\\n",
    "                       ,StructField(\"redirects_page\", StringType(), True)])\n",
    "\n",
    "   \n",
    "    data = [{'page_id':lst_page_id,'page_title':lst_page_titles, 'page_text': lst_texts, 'category': lst_categories, 'links': lst_links, \n",
    "             'external_links': lst_externallinks, 'sections': lst_sections, 'redirects_page': lst_redirects} \n",
    "            for lst_page_id,lst_page_titles,lst_texts,lst_categories, lst_links, lst_externallinks, lst_sections, lst_redirects\n",
    "            in zip(lst_page_id,lst_page_titles,lst_texts,lst_categories, lst_links, lst_externallinks, lst_sections, lst_redirects)]\n",
    "\n",
    "    #df = spark.createDataFrame(Row(**x) for x in data, schema = mySchema)\n",
    "    df = spark.createDataFrame(data, schema = mySchema)\n",
    "    df = df.where(~sparkfn.array_contains(df.category, 'Disambiguation_pages'))\n",
    "    df = df.where(~sparkfn.array_contains(df.category, 'Disambiguation pages'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/srv/spark/shared-data-export/liuy/datasets/video_level_classification/Video_game_extraction/Access_wikidata'\n",
    "with open(os.path.join(root, 'wikiListOfArticles_samp.txt')) as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "samp_titles = [x.split('; ')[1].strip() for x in content]\n",
    "samp_titles = samp_titles[:1000]\n",
    "len(samp_titles)\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.58306884765625e-06\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    Len_titles = len(samp_titles) # 数据集长度\n",
    "    num_cores = cpu_count()   # cpu核心数\n",
    "    results = []\n",
    "    pool = Pool(num_cores)\n",
    "    \n",
    "    chunks = list(split(samp_titles, 10))\n",
    "    stime = time.time()\n",
    "    print(time.time()-stime)\n",
    "    #p.apply_async(single_worker, args=(List_subsets[i], SourceImgs, HE_path))\n",
    "    result = pool.map(crawler,chunks) # 这行代码表示所有的进行都已经执行完了，并且每个进程的结果都拿到，放在了res中 #l是foo()的参数\n",
    "    print(time.time()-stime) \n",
    "    results.extend(result)\n",
    "    #print(res,type(res)) #res 存储结果\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(time.time()-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1.0967254638671875e-05\n",
      "0.00019431114196777344\n",
      "62.00026869773865\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    num_cores = cpu_count()   # cpu核心数\n",
    "    #init objects\n",
    "    pool = Pool(num_cores)\n",
    "    jobs = []\n",
    "    \n",
    "    chunks = list(split(samp_titles, 10))\n",
    "    print(len(chunks))\n",
    "    stime = time.time()\n",
    "    print(time.time()-stime)\n",
    "\n",
    "    for mini_chunk in chunks:\n",
    "    #\n",
    "    # 格式：p.apply_async(task, args=(...))\n",
    "    # task：当前进程需要进行的任务/函数，只需要填写函数名\n",
    "    # args：task函数中所需要传入的参数\n",
    "    # 注意看 List_subsets[i] 就是传入不同的数据子集\n",
    "        jobs.append(pool.apply_async(crawler, args=(mini_chunk,)))\n",
    "    print(time.time()-stime) \n",
    "    \n",
    "    #wait for all jobs to finish\n",
    "#     for job in jobs:\n",
    "#         job.get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(time.time()-stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#理解map() and apply_asyn()\n",
    "\n",
    "pool.apply(f, args): f is only executed in ONE of the workers of the pool. So ONE of the processes in the pool will run f(args).\n",
    "only one of the workers execute the function, you have to call apply multiple times to have the rest of the workers perform a task.\n",
    "\n",
    "apply() can only be called for one job, func 仅在池中的一个工作程序中执行。\n",
    "***********\n",
    "\n",
    "pool.map(f, iterable): This method chops the iterable into a number of chunks which it submits to the process pool as separate tasks. So you take advantage of all the processes in the pool.\n",
    "\n",
    "map() block the main process until all the processes complete and return the result.\n",
    "map is called for a list of jobs in one time\n",
    "\n",
    "when using map(), put 13 arguments in one iterable and run it with 12 core. 12 jobs were running simultaneously and the rest one works after the first bunch is done.\n",
    "\n",
    "此方法将iterable内的每一个对象作为单独的任务提交给进程池。可以通过将chunksize设置为正整数来指定这些块的（近似）大小。\n",
    "****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "global interpreter lock (GIL).This means that only one thread can execute at a time, since the interpreter blocks access for all other threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x):\n",
    "    s0 = time()\n",
    "    res = 0\n",
    "    for _ in range(x*1000000):\n",
    "        res += 1\n",
    "    print(mp.current_process(),'run time:%.3f s, result:%.1f'%(time()-s0,res))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing.Pool可以提供指定数量的进程供用户调用，当有新的请求提交到pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
